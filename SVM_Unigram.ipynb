{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Set the environment"
      ],
      "metadata": {
        "id": "s38qJXs5an2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CWdFFsxURMhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db792f91-5306-49e9-ec13-a13c2b2ab241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install requirements.txt.\n",
        "#Given path might not work for you as your location might be different\n",
        "#Find the requirements.txt and copy path and replace this path\n",
        "\n",
        "!pip install -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt"
      ],
      "metadata": {
        "id": "T3rZVKSFRyTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da18007-3344-4350-fae1-5f95778282c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting appdirs (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 1))\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting astroid (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 2))\n",
            "  Downloading astroid-4.0.3-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 3)) (4.13.5)\n",
            "Collecting bs4 (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 4))\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: CacheControl in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 5)) (0.14.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 6)) (2025.11.12)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 7)) (5.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 8)) (8.3.1)\n",
            "Collecting colorama (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 9))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting contextlib2 (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 10))\n",
            "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting distlib (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 11))\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 12)) (1.9.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 13)) (1.0.0)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 14)) (1.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 15)) (3.11)\n",
            "Collecting ipaddr (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 16))\n",
            "  Downloading ipaddr-2.2.0.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting isort (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 17))\n",
            "  Downloading isort-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 18)) (1.5.3)\n",
            "Collecting lazy-object-proxy (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 19))\n",
            "  Downloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting lockfile (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 20))\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting mccabe (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 21))\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 22)) (1.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 23)) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 24)) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 25)) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 26)) (2.2.2)\n",
            "Collecting pep517 (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 27))\n",
            "  Downloading pep517-0.13.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting progress (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 28))\n",
            "  Downloading progress-1.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pylint (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 29))\n",
            "  Downloading pylint-4.0.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 30)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 31)) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 32)) (8.0.4)\n",
            "Collecting pytoml (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 33))\n",
            "  Downloading pytoml-0.1.21-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 34)) (2025.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 35)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 36)) (2.32.4)\n",
            "Collecting retrying (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 37))\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 38)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 39)) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 40)) (1.17.0)\n",
            "Collecting slugify (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 41))\n",
            "  Downloading slugify-0.0.1.tar.gz (1.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 42)) (2.8)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 43)) (1.3)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 44)) (3.6.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 45)) (0.10.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 47)) (4.67.1)\n",
            "Collecting typed-ast (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 48))\n",
            "  Downloading typed_ast-1.5.5.tar.gz (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 49)) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 50)) (0.5.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 51)) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 3)) (4.15.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 26)) (2025.3)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.12/dist-packages (from pylint->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 29)) (0.3.8)\n",
            "Requirement already satisfied: platformdirs>=2.2 in /usr/local/lib/python3.12/dist-packages (from pylint->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 29)) (4.5.1)\n",
            "Requirement already satisfied: tomlkit>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from pylint->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 29)) (0.13.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 36)) (3.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r /content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/requirements.txt (line 46)) (3.0.3)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading astroid-4.0.3-py3-none-any.whl (276 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isort-7.0.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading pep517-0.13.1-py3-none-any.whl (19 kB)\n",
            "Downloading progress-1.6.1-py3-none-any.whl (9.8 kB)\n",
            "Downloading pylint-4.0.4-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.4/536.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytoml-0.1.21-py2.py3-none-any.whl (8.5 kB)\n",
            "Downloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: ipaddr, slugify, typed-ast\n",
            "  Building wheel for ipaddr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipaddr: filename=ipaddr-2.2.0-py3-none-any.whl size=18281 sha256=90d21c8851d26e8f477d483eb8853bcfe6256e2179131e75fb985d4e37a959e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/99/bd/70fe0c01fa82e66ac6e05659f7d1c8bd61770d582c63c36824\n",
            "  Building wheel for slugify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for slugify: filename=slugify-0.0.1-py3-none-any.whl size=1882 sha256=247ee18bb418ae3dda4eb35eb5c4f838636f8c877e4720e0a454052496574d16\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/3e/6c/a4bc4216d0ab5d04bc72d575ffc4372cb3b6aabf8982d28ab4\n",
            "  Building wheel for typed-ast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typed-ast: filename=typed_ast-1.5.5-cp312-cp312-linux_x86_64.whl size=816865 sha256=23f777df89d751e7029c8b7d7f95efe502071b50d544040a9aaa30ab5b822774\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/85/f8/cb309265bf60f840369ca24241385ae985caab44af05272b6a\n",
            "Successfully built ipaddr slugify typed-ast\n",
            "Installing collected packages: slugify, pytoml, lockfile, ipaddr, distlib, appdirs, typed-ast, retrying, progress, pep517, mccabe, lazy-object-proxy, isort, contextlib2, colorama, astroid, pylint, bs4\n",
            "Successfully installed appdirs-1.4.4 astroid-4.0.3 bs4-0.0.2 colorama-0.4.6 contextlib2-21.6.0 distlib-0.4.0 ipaddr-2.2.0 isort-7.0.0 lazy-object-proxy-1.12.0 lockfile-0.12.2 mccabe-0.7.0 pep517-0.13.1 progress-1.6.1 pylint-4.0.4 pytoml-0.1.21 retrying-1.4.2 slugify-0.0.1 typed-ast-1.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These are all the imports necessary. If you encounter anything missing just import here (hopefully that won't be the case).\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string\n",
        "import pickle\n",
        "import torch\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "import sys\n",
        "\n",
        "#sys.append Basic folder and Helper folder. Both can be found in the Models subfolder. copy your path and replace it.\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/Models/Basic')\n",
        "sys.path.append('/content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/Models/Helper')\n",
        "\n",
        "import load_feature\n",
        "import helper"
      ],
      "metadata": {
        "id": "d8plBL-eSInr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change our directory where n-gram.py is located.\n",
        "#Given path might not work for you as your location might be different\n",
        "#Find the \"Basic\" sub-folder in the \"Models\" folder where n-gram.py is located and replace this path\n",
        "\n",
        "%cd drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/Models/Basic"
      ],
      "metadata": {
        "id": "nhc0eHg4TGbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918e79d7-4693-41ff-b900-7cd3db346eaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/Models/Basic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train model"
      ],
      "metadata": {
        "id": "NmgCkfB3awWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run our model\n",
        "#Change exp_name and model_name according to your work\n",
        "#For example : \" !python n-gram.py Unigram SVM -s \"\"\n",
        "#Refer to https://github.com/Rowan1697/FakeNews for all the available experiment names and models\n",
        "\n",
        "!python n-gram.py Unigram SVM -s\n",
        "\n",
        "#After your model has been trained you can find the saved RESULT in the results folder\n",
        "#After your model has been trained you can find the saved MODEL in the saved-model folder\n",
        "#Your saved model will have the naming convenction of \"model name\" + '_' + \"experiment name\" + \".pkl\"\n",
        "#For example \"LR_C3-gram.pkl\""
      ],
      "metadata": {
        "id": "B4WDPxlrTdyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bf037e-b5ed-4b24-de41-298ef9042316"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['পাওয়া', 'পেয়ে', 'বিষয়টি', 'হইয়া', 'হয়েছিল'] not in stop_words.\n",
            "  warnings.warn(\n",
            "                                Weighted Avg         #            Non-Violence           #          Passive Violence         #          Active Violence          \n",
            "                   precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score\n",
            "Unigram            0.72         0.72        0.72      #  0.73         0.80         0.76      #  0.71         0.67         0.69      #  0.73         0.66         0.69\n",
            "                                Macro Avg            #            Non-Violence           #          Passive Violence         #          Active Violence          \n",
            "                   precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score\n",
            "Unigram            0.72         0.71        0.72      #  0.73         0.80         0.76      #  0.71         0.67         0.69      #  0.73         0.66         0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "R6obynsZbmp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set the path of our model and datasets\n",
        "#You can find the models in saved models folder under Basic\n",
        "#You can find the datasets in dataset folder under 4-2\n",
        "\n",
        "model_path = '/content/drive/MyDrive/IndoNLP-main/IndoNLP-main/FakeNews-master/Models/Basic/Saved Models/SVM_Unigram.pkl'\n",
        "train_data_path = '/content/drive/MyDrive/IndoNLP-main/IndoNLP-main/Datasets/train.csv'\n",
        "test_data_path = '/content/drive/MyDrive/IndoNLP-main/IndoNLP-main/Datasets/test.csv'"
      ],
      "metadata": {
        "id": "rFI-391wUNcm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the test dataset\n",
        "\n",
        "df_train = pd.read_csv(train_data_path)\n",
        "df_test = pd.read_csv(test_data_path)\n",
        "\n",
        "# Rename columns to match what load_feature.py expects\n",
        "# Your columns: 'Text' → rename to 'Content'\n",
        "#               'Labels' → rename to 'Label' (singular, as used in word_emb)\n",
        "df_train = df_train.rename(columns={'text': 'Content', 'label': 'Label'})\n",
        "df_test = df_test.rename(columns={'text': 'Content', 'label': 'Label'})\n",
        "\n",
        "# Now create X_train and X_test as DataFrames with 'Content' column\n",
        "X_train = df_train[['Content']]\n",
        "X_test = df_test[['Content']]\n",
        "\n",
        "y_test = df_test['Label'].values.ravel()\n",
        "\n",
        "#For Unigram set a = 1, b = 1\n",
        "#For Bigram set a = 2, b = 2\n",
        "#For Trigram set a = 3, b = 3\n",
        "#For U+B+T set a = 1, b = 3\n",
        "\n",
        "#For C3-gram set a = 3, b = 3\n",
        "#For C4-gram set a = 4, b = 4\n",
        "#For C5-gram set a = 5, b = 5\n",
        "#For C3+C4+C5 set a = 3, b = 5\n",
        "\n",
        "a = 1\n",
        "b = 1\n",
        "\n",
        "#If you are using U/B/T : use wordF\n",
        "#If you are using C3/C4/C5 : use charF\n",
        "#For example when using C3, I have only used charF, and commented out wordF.\n",
        "\n",
        "X_test = load_feature.tfidf_wordF(X_train, X_test, a, b)\n",
        "# X_test = load_feature.tfidf_charF(X_train, X_test, a, b)"
      ],
      "metadata": {
        "id": "i1nGh0dlhz6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c712c9e0-ddc5-44d9-a1d9-2094f5306c46"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['পাওয়া', 'পেয়ে', 'বিষয়টি', 'হইয়া', 'হয়েছিল'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load and Test the model\n",
        "\n",
        "clf = pickle.load(open(model_path, 'rb'))\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "9O77SPHOiwrh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #get and print the results\n",
        "# t, f, o, om = helper.getResult(y_test, y_pred)\n",
        "\n",
        "# print(\"                                Overall               #               Fake                \")\n",
        "# print(\"                   precision    recall      f1-score  #  precision    recall      f1-score\")\n",
        "\n",
        "# #Set the 'model' and 'experiment' names\n",
        "# model = 'SVM'\n",
        "# experiment = 'Unigram'\n",
        "# exp = model+'_'+experiment\n",
        "\n",
        "# res = helper.printResult(exp,o,f)\n",
        "# print(res)\n",
        "\n",
        "# path = model+\"_final_results.txt\"\n",
        "# helper.saveResults(path, res)\n",
        "\n",
        "# print(\"                                Overall               #               Fake                \")\n",
        "# print(\"                   precision    recall      f1-score  #  precision    recall      f1-score\")\n",
        "\n",
        "# res = helper.printResult(exp,om,f)\n",
        "# print(res)\n",
        "\n",
        "# path = model+\"_final_results_micro.txt\"\n",
        "# helper.saveResults(path, res)"
      ],
      "metadata": {
        "id": "I0CX-mM4jI_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load and Test the model\n",
        "\n",
        "clf = pickle.load(open(model_path, 'rb'))\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "class0, class1, class2, o_weighted, o_macro = helper.getResult(y_test, y_pred)\n",
        "\n",
        "print(\"                                Weighted Avg         #            Non-Violence           #          Passive Violence         #          Active Violence          \")\n",
        "print(\"                   precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score\")\n",
        "\n",
        "#Set the 'model' and 'experiment' names\n",
        "model = 'SVM'\n",
        "experiment = 'Unigram'\n",
        "exp = model+'_'+experiment\n",
        "\n",
        "res = helper.printResult(exp, o_weighted, class0, class1, class2)\n",
        "print(res)\n",
        "\n",
        "path = model+\"_final_results.txt\"\n",
        "helper.saveResults(path, res)\n",
        "\n",
        "print(\"                                Macro Avg            #            Non-Violence           #          Passive Violence         #          Active Violence          \")\n",
        "print(\"                   precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score\")\n",
        "\n",
        "res = helper.printResult(exp, o_macro, class0, class1, class2)\n",
        "print(res)\n",
        "\n",
        "path = model+\"_final_results_macro.txt\"\n",
        "helper.saveResults(path, res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkndNp835XlO",
        "outputId": "c2df28c2-2bd5-490c-9f1e-626a28f4f184"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                Weighted Avg         #            Non-Violence           #          Passive Violence         #          Active Violence          \n",
            "                   precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score\n",
            "SVM_Unigram        0.73         0.73        0.73      #  0.75         0.80         0.77      #  0.71         0.68         0.69      #  0.73         0.67         0.70\n",
            "                                Macro Avg            #            Non-Violence           #          Passive Violence         #          Active Violence          \n",
            "                   precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score  #  precision    recall      f1-score\n",
            "SVM_Unigram        0.73         0.72        0.72      #  0.75         0.80         0.77      #  0.71         0.68         0.69      #  0.73         0.67         0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yGLkJQ8W7Upz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}